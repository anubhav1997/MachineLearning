{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least Squares\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,random_state=0)\n",
    "\n",
    "model=LinearRegression()\n",
    "model.fit(x_train,y_train)\n",
    "\n",
    "print('Linear Model Coefficient:', model.coef_)\n",
    "print('Linear Model Intercept:', model.intercept_)\n",
    "print('R-Squared Score(Training):', model.score(x_train,y_train))\n",
    "print('R-Squared Sccore(Test):', model.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Ridge Regression\n",
    "\n",
    "It includes a penalty term to sum of squares for variations. It will prefer a model with lower sum of feature weights(penalty). It is good for data with larger features. Penalty can be controlled by alpha paramter  aka regularization parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "model=Ridge(alpha=0.20).fit(x_train,y_train)\n",
    "\n",
    "print('Ridge Model Coefficient:', model.coef_)\n",
    "print('Ridge Model Intercept:', model.intercept_)\n",
    "print('Score(Training):', model.score(x_train,y_train))\n",
    "print('Sccore(Test):', model.score(x_test,y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve performance we may have to normalize features. This will include scaling the features to the same scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn,preprocessing import MinMaxScaler\n",
    "\n",
    "scaler=MinMaxScaler()\n",
    "scaler.fit(x_train)\n",
    "x_train_scaled=scaler.transform(x_train)\n",
    "x_test_scaled=scaler.transform(x_test)\n",
    "\n",
    "model=Ridge().fit(x_train_scaled,y_train)\n",
    "score=model.fit(x_test_scaled,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regression\n",
    "\n",
    "Lasso also uses a penalty like ridge but it uses a different regularization parameter. Instead of calculating sum of squares of wiehts or penalties it calculates the absolute sum of the penalties or weights.\n",
    "\n",
    "Lasso is used when the variables are small and have a large effect on the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn,preprocessing import MinMaxScaler\n",
    "\n",
    "scaler=MinMaxScaler()\n",
    "scaler.fit(x_train)\n",
    "x_train_scaled=scaler.transform(x_train)\n",
    "x_test_scaled=scaler.transform(x_test)\n",
    "\n",
    "model=Lasso(alpha=0.20,max_iter=10000).fit(x_train_scaled,y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
